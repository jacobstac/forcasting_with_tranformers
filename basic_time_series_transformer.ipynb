{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_time_series_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGJ5IP8jazqAx87iecusEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobstac/forcasting_with_tranformers/blob/main/basic_time_series_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNUOQZu7j7vS"
      },
      "source": [
        "import torch.nn as nn, torch\n",
        "from pprint import pprint\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_p8vdJEkHdE",
        "outputId": "e22f1efd-edb2-4ecd-a1aa-1bd1f15b39a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#data 1\n",
        "dataset = np.array([\n",
        "  [([0,1,0,1,0,1]), ([[0], [1]])],\n",
        "  [([1,0,1,0,1,0]), ([[1], [0]])],\n",
        "  [([0,0,0,0,0,0]), ([[0], [0]])],\n",
        "  [([1,1,1,1,1,1]), ([[1], [1]])],\n",
        "  [([1,1,0,0,1,1]), ([[0], [0]])],\n",
        "  [([0,0,1,1,0,0]), ([[1], [1]])]\n",
        "])\n",
        "print(dataset.shape)\n",
        "seq_len = len(dataset[0][0])\n",
        "print(seq_len)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 2)\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEYlDjIGkLrp",
        "outputId": "48f28d91-3787-4c31-fd4c-0c2655e894f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset[0][0], dataset[0][1]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 0, 1, 0, 1], [[0], [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_A_UL_SkR7W"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embed_src    = nn.Embedding(seq_len, 10)  # 2 options 0 or 1\n",
        "    self.embed_target = nn.Embedding(seq_len, 10)  # 2 options 0 or 1\n",
        "    self.transformer = nn.Transformer(10, 2) # embedding size as first argument\n",
        "    self.lin = nn.Linear(10, 2)              # reduce from 10 to 2\n",
        "    self.softmax = nn.Softmax(dim=-1)         # for displaying probability\n",
        "    \n",
        "  def forward(self, inp, tgt):\n",
        "    embed_src = self.embed_src(inp)\n",
        "    embed_target = self.embed_target(tgt)\n",
        "    \n",
        "    output = self.transformer(embed_src.view(len(inp), 1, -1), embed_target.view(len(tgt), 1, -1))\n",
        "    # transformer takes seq len,batch size, embedding size of src and tgt seq\n",
        "    \n",
        "    output = self.lin(output)\n",
        "    \n",
        "    print(\"output.shape: \", output.shape, \"output\", output)\n",
        "    print(\"embed_target: \", embed_target.view(2,1, -1))\n",
        "    print(\"softmax prob: \", self.softmax(output))\n",
        "    \n",
        "    return output.permute(0, 2, 1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g0y6oV6kX3a"
      },
      "source": [
        "model = TransformerModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbGkkFDbkb1_",
        "outputId": "f82add40-021d-40c9-b1ce-6111f20461b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tensor_dataset = []\n",
        "\n",
        "print(dataset[0][0])\n",
        "for i in range (seq_len):\n",
        "  tensor_dataset.append([torch.tensor(dataset[i][0]), torch.tensor(dataset[i][1])])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcztTMLzkd6V",
        "outputId": "5b1c17e2-dd2c-45bf-a20b-b5f85d755405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(tensor_dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[tensor([0, 1, 0, 1, 0, 1]), tensor([[0],\n",
            "        [1]])],\n",
            " [tensor([1, 0, 1, 0, 1, 0]), tensor([[1],\n",
            "        [0]])],\n",
            " [tensor([0, 0, 0, 0, 0, 0]), tensor([[0],\n",
            "        [0]])],\n",
            " [tensor([1, 1, 1, 1, 1, 1]), tensor([[1],\n",
            "        [1]])],\n",
            " [tensor([1, 1, 0, 0, 1, 1]), tensor([[0],\n",
            "        [0]])],\n",
            " [tensor([0, 0, 1, 1, 0, 0]), tensor([[1],\n",
            "        [1]])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_btaCQFke7Z"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUcdIKr_kiOx",
        "outputId": "7d9c0abc-9f47-4637-8e8f-3770d9ec82b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  print(tensor_dataset[i % seq_len][0], tensor_dataset[i % seq_len][1])\n",
        "  loss = criterion(model(tensor_dataset[i % seq_len][0], tensor_dataset[i % seq_len][1]), tensor_dataset[i % seq_len][1])\n",
        "  print(\"loss: \", loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1208, -2.7316]],\n",
            "\n",
            "        [[-2.0052,  2.4762]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9923, 0.0077]],\n",
            "\n",
            "        [[0.0112, 0.9888]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0095, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1521,  2.0348]],\n",
            "\n",
            "        [[ 1.9392, -2.6601]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0150, 0.9850]],\n",
            "\n",
            "        [[0.9900, 0.0100]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0125, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2015, -2.5762]],\n",
            "\n",
            "        [[ 2.3167, -2.8621]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9917, 0.0083]],\n",
            "\n",
            "        [[0.9944, 0.0056]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0070, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2171,  2.2495]],\n",
            "\n",
            "        [[-2.2228,  2.4867]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0114, 0.9886]],\n",
            "\n",
            "        [[0.0089, 0.9911]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0102, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4014, -2.6463]],\n",
            "\n",
            "        [[ 2.4011, -2.5061]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9936, 0.0064]],\n",
            "\n",
            "        [[0.9927, 0.0073]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0069, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.8886,  2.5102]],\n",
            "\n",
            "        [[-2.1014,  2.2646]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0121, 0.9879]],\n",
            "\n",
            "        [[0.0125, 0.9875]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0124, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 1.6448, -2.6708]],\n",
            "\n",
            "        [[-2.0305,  2.5299]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6432]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9868, 0.0132]],\n",
            "\n",
            "        [[0.0104, 0.9897]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0118, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1134,  2.4738]],\n",
            "\n",
            "        [[ 1.3444, -2.5042]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9006, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0482,\n",
            "          -1.0365, -1.7740, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0101, 0.9899]],\n",
            "\n",
            "        [[0.9791, 0.0209]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0156, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 1.7939, -2.8752]],\n",
            "\n",
            "        [[ 2.1860, -2.8713]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6378,  0.4199, -0.0481,\n",
            "          -1.0365, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6378,  0.4199, -0.0481,\n",
            "          -1.0365, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9907, 0.0093]],\n",
            "\n",
            "        [[0.9937, 0.0063]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0078, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2801,  2.4313]],\n",
            "\n",
            "        [[-1.7698,  2.1913]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0089, 0.9911]],\n",
            "\n",
            "        [[0.0187, 0.9813]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0139, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3000, -2.7772]],\n",
            "\n",
            "        [[ 2.1391, -2.7475]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6378,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6378,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9938, 0.0062]],\n",
            "\n",
            "        [[0.9925, 0.0075]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0069, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1790,  2.4602]],\n",
            "\n",
            "        [[-2.3630,  2.3807]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0096, 0.9904]],\n",
            "\n",
            "        [[0.0086, 0.9914]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0091, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.0875, -2.8704]],\n",
            "\n",
            "        [[-2.0970,  2.4671]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6378,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9930, 0.0070]],\n",
            "\n",
            "        [[0.0103, 0.9897]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0087, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2041,  2.5322]],\n",
            "\n",
            "        [[ 2.0841, -2.7547]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0087, 0.9913]],\n",
            "\n",
            "        [[0.9921, 0.0079]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0083, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4547, -2.8315]],\n",
            "\n",
            "        [[ 1.9771, -2.5869]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9950, 0.0050]],\n",
            "\n",
            "        [[0.9897, 0.0103]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0077, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0058,  2.1942]],\n",
            "\n",
            "        [[-1.8401,  2.5525]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0148, 0.9852]],\n",
            "\n",
            "        [[0.0122, 0.9878]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0136, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1429, -2.4538]],\n",
            "\n",
            "        [[ 1.9983, -2.5992]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9900, 0.0100]],\n",
            "\n",
            "        [[0.9900, 0.0100]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0100, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0909,  2.4237]],\n",
            "\n",
            "        [[-2.2179,  2.4272]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0108, 0.9892]],\n",
            "\n",
            "        [[0.0095, 0.9905]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0102, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 1.6202, -2.4872]],\n",
            "\n",
            "        [[-2.1712,  2.3799]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9838, 0.0162]],\n",
            "\n",
            "        [[0.0104, 0.9896]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0134, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2345,  2.4921]],\n",
            "\n",
            "        [[ 2.0802, -2.8236]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0088, 0.9912]],\n",
            "\n",
            "        [[0.9926, 0.0074]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0081, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2167, -2.5808]],\n",
            "\n",
            "        [[ 2.0546, -2.5584]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9918, 0.0082]],\n",
            "\n",
            "        [[0.9902, 0.0098]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0090, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2692,  2.3926]],\n",
            "\n",
            "        [[-2.3126,  2.3869]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0094, 0.9906]],\n",
            "\n",
            "        [[0.0090, 0.9910]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0092, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2319, -2.9147]],\n",
            "\n",
            "        [[ 2.4287, -2.7120]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9942, 0.0058]],\n",
            "\n",
            "        [[0.9942, 0.0058]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0058, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0445,  2.3952]],\n",
            "\n",
            "        [[-2.2741,  2.1764]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0117, 0.9883]],\n",
            "\n",
            "        [[0.0115, 0.9885]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0117, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.0580, -2.8848]],\n",
            "\n",
            "        [[-1.9270,  2.4517]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9929, 0.0071]],\n",
            "\n",
            "        [[0.0124, 0.9876]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0098, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1577,  2.2771]],\n",
            "\n",
            "        [[ 2.3465, -2.9162]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0117, 0.9883]],\n",
            "\n",
            "        [[0.9948, 0.0052]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0085, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3740, -2.7418]],\n",
            "\n",
            "        [[ 1.9325, -2.7664]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9940, 0.0060]],\n",
            "\n",
            "        [[0.9910, 0.0090]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0075, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0084,  2.4095]],\n",
            "\n",
            "        [[-2.2716,  2.3945]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0119, 0.9881]],\n",
            "\n",
            "        [[0.0093, 0.9907]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0107, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.0967, -2.9347]],\n",
            "\n",
            "        [[ 1.4555, -2.0211]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9935, 0.0065]],\n",
            "\n",
            "        [[0.9700, 0.0300]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0185, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.9804,  2.5738]],\n",
            "\n",
            "        [[-2.1813,  2.4358]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0104, 0.9896]],\n",
            "\n",
            "        [[0.0098, 0.9902]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0102, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.0206, -2.6136]],\n",
            "\n",
            "        [[-2.0571,  2.4923]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9904, 0.0096]],\n",
            "\n",
            "        [[0.0105, 0.9895]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0101, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0209,  2.4764]],\n",
            "\n",
            "        [[ 2.3674, -2.8598]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0110, 0.9890]],\n",
            "\n",
            "        [[0.9947, 0.0053]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0082, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4157, -2.9022]],\n",
            "\n",
            "        [[ 2.2953, -2.6700]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9951, 0.0049]],\n",
            "\n",
            "        [[0.9931, 0.0069]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0059, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2906,  2.2943]],\n",
            "\n",
            "        [[-2.2311,  2.5584]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0101, 0.9899]],\n",
            "\n",
            "        [[0.0082, 0.9918]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0092, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4130, -2.5308]],\n",
            "\n",
            "        [[ 2.3730, -2.5851]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9929, 0.0071]],\n",
            "\n",
            "        [[0.9930, 0.0070]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0071, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3284,  2.4527]],\n",
            "\n",
            "        [[-2.3574,  2.3370]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0083, 0.9917]],\n",
            "\n",
            "        [[0.0091, 0.9909]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0087, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 1.8058, -2.4659]],\n",
            "\n",
            "        [[-1.9426,  2.5981]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9862, 0.0138]],\n",
            "\n",
            "        [[0.0106, 0.9894]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0122, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.9406,  2.2650]],\n",
            "\n",
            "        [[ 2.3470, -2.6306]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8305,  2.0160,  0.6666, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0147, 0.9853]],\n",
            "\n",
            "        [[0.9932, 0.0068]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0108, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2328, -2.7593]],\n",
            "\n",
            "        [[ 2.4522, -2.8533]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9933, 0.0067]],\n",
            "\n",
            "        [[0.9951, 0.0049]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0059, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3179,  2.3859]],\n",
            "\n",
            "        [[-2.2292,  2.5279]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0090, 0.9910]],\n",
            "\n",
            "        [[0.0085, 0.9915]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0088, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3438, -2.1584]],\n",
            "\n",
            "        [[ 2.2942, -2.7453]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9890, 0.0110]],\n",
            "\n",
            "        [[0.9936, 0.0064]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0087, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2921,  2.2302]],\n",
            "\n",
            "        [[-2.2534,  2.4390]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0107, 0.9893]],\n",
            "\n",
            "        [[0.0091, 0.9909]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0100, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1512, -2.9929]],\n",
            "\n",
            "        [[-2.2707,  2.4388]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3171, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]],\n",
            "\n",
            "        [[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9942, 0.0058]],\n",
            "\n",
            "        [[0.0089, 0.9911]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0074, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-0.8918,  0.6781]],\n",
            "\n",
            "        [[ 2.2624, -2.7109]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0873, -0.6005,  0.8306,  2.0160,  0.6665, -1.5324,  0.1599,\n",
            "          -0.9005, -0.0457,  0.9033]],\n",
            "\n",
            "        [[ 0.3170, -1.2659, -1.7323, -0.3029,  0.6377,  0.4199, -0.0481,\n",
            "          -1.0364, -1.7741, -1.6431]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.1722, 0.8278]],\n",
            "\n",
            "        [[0.9931, 0.0069]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0980, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1504, -2.0500]],\n",
            "\n",
            "        [[ 2.2711, -1.9728]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2638, -1.7316, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0348, -1.7766, -1.6409]],\n",
            "\n",
            "        [[ 0.3170, -1.2638, -1.7316, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0348, -1.7766, -1.6409]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9852, 0.0148]],\n",
            "\n",
            "        [[0.9859, 0.0141]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0146, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2519,  2.5056]],\n",
            "\n",
            "        [[-2.1584,  2.3213]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]],\n",
            "\n",
            "        [[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0085, 0.9915]],\n",
            "\n",
            "        [[0.0112, 0.9888]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0099, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2425, -2.3757]],\n",
            "\n",
            "        [[ 2.1643, -2.7159]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2638, -1.7317, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0349, -1.7765, -1.6409]],\n",
            "\n",
            "        [[ 0.3170, -1.2638, -1.7317, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0349, -1.7765, -1.6409]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9902, 0.0098]],\n",
            "\n",
            "        [[0.9925, 0.0075]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0087, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2708,  2.5257]],\n",
            "\n",
            "        [[-2.0157,  2.1285]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]],\n",
            "\n",
            "        [[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0082, 0.9918]],\n",
            "\n",
            "        [[0.0156, 0.9844]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0120, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 1.9737, -2.6651]],\n",
            "\n",
            "        [[-2.2975,  2.4574]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3170, -1.2638, -1.7317, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0348, -1.7765, -1.6409]],\n",
            "\n",
            "        [[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9904, 0.0096]],\n",
            "\n",
            "        [[0.0085, 0.9915]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0091, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0933,  2.3649]],\n",
            "\n",
            "        [[ 0.6503, -0.1401]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.0980, -0.6029,  0.8442,  2.0118,  0.6642, -1.5389,  0.1526,\n",
            "          -0.9024, -0.0405,  0.9087]],\n",
            "\n",
            "        [[ 0.3170, -1.2638, -1.7317, -0.3016,  0.6333,  0.4184, -0.0430,\n",
            "          -1.0348, -1.7765, -1.6409]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0115, 0.9885]],\n",
            "\n",
            "        [[0.6879, 0.3121]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.1928, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3236, -2.2530]],\n",
            "\n",
            "        [[ 2.4360, -2.6371]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7396, -0.3164,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7396, -0.3164,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9898, 0.0102]],\n",
            "\n",
            "        [[0.9938, 0.0062]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0082, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.5599,  2.4041]],\n",
            "\n",
            "        [[-1.9204,  2.5398]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1023, -0.5993,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]],\n",
            "\n",
            "        [[-0.1023, -0.5993,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0186, 0.9814]],\n",
            "\n",
            "        [[0.0114, 0.9886]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0152, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.0042, -2.2153]],\n",
            "\n",
            "        [[ 2.3584, -2.3984]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7396, -0.3164,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7396, -0.3164,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9855, 0.0145]],\n",
            "\n",
            "        [[0.9915, 0.0085]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0116, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1873,  2.3844]],\n",
            "\n",
            "        [[-1.8609,  2.5373]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]],\n",
            "\n",
            "        [[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0102, 0.9898]],\n",
            "\n",
            "        [[0.0121, 0.9879]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0113, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3648, -2.7844]],\n",
            "\n",
            "        [[-2.0338,  2.5391]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7396, -0.3163,  0.6259,  0.4212, -0.0457,\n",
            "          -1.0306, -1.7717, -1.6421]],\n",
            "\n",
            "        [[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9942, 0.0058]],\n",
            "\n",
            "        [[0.0102, 0.9898]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0080, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.8399,  2.1326]],\n",
            "\n",
            "        [[ 1.8424, -2.1654]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9054, -0.0422,  0.9124]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7396, -0.3163,  0.6259,  0.4212, -0.0457,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0185, 0.9815]],\n",
            "\n",
            "        [[0.9822, 0.0178]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0183, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4637, -2.5703]],\n",
            "\n",
            "        [[ 2.1487, -2.4956]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9935, 0.0065]],\n",
            "\n",
            "        [[0.9905, 0.0095]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0080, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.4371,  2.4316]],\n",
            "\n",
            "        [[-1.8031,  2.5516]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]],\n",
            "\n",
            "        [[-0.1023, -0.5994,  0.8419,  2.0117,  0.6631, -1.5420,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0205, 0.9795]],\n",
            "\n",
            "        [[0.0127, 0.9873]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0167, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4028, -2.9026]],\n",
            "\n",
            "        [[ 2.3801, -2.3288]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9951, 0.0049]],\n",
            "\n",
            "        [[0.9911, 0.0089]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0070, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1961,  2.5594]],\n",
            "\n",
            "        [[-2.0457,  2.5678]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1022, -0.5994,  0.8419,  2.0117,  0.6631, -1.5419,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]],\n",
            "\n",
            "        [[-0.1022, -0.5994,  0.8419,  2.0117,  0.6631, -1.5419,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0085, 0.9915]],\n",
            "\n",
            "        [[0.0098, 0.9902]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0092, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3017, -2.6836]],\n",
            "\n",
            "        [[-1.6383,  2.3245]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7718, -1.6421]],\n",
            "\n",
            "        [[-0.1022, -0.5994,  0.8419,  2.0117,  0.6631, -1.5419,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9932, 0.0068]],\n",
            "\n",
            "        [[0.0187, 0.9813]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0128, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.6678,  2.1623]],\n",
            "\n",
            "        [[ 2.1186, -2.7204]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1022, -0.5994,  0.8419,  2.0118,  0.6631, -1.5419,  0.1488,\n",
            "          -0.9053, -0.0422,  0.9124]],\n",
            "\n",
            "        [[ 0.3151, -1.2575, -1.7395, -0.3163,  0.6259,  0.4212, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0212, 0.9788]],\n",
            "\n",
            "        [[0.9921, 0.0079]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0147, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4960, -2.4186]],\n",
            "\n",
            "        [[ 2.3070, -2.7750]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9927, 0.0073]],\n",
            "\n",
            "        [[0.9938, 0.0062]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0068, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1503,  2.5378]],\n",
            "\n",
            "        [[-2.2897,  2.5504]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0423,  0.9124]],\n",
            "\n",
            "        [[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0423,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0091, 0.9909]],\n",
            "\n",
            "        [[0.0078, 0.9922]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0085, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4534, -2.9084]],\n",
            "\n",
            "        [[ 2.3804, -2.9673]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]],\n",
            "\n",
            "        [[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9953, 0.0047]],\n",
            "\n",
            "        [[0.9953, 0.0047]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0047, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1860,  2.5043]],\n",
            "\n",
            "        [[-2.0662,  2.5439]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0423,  0.9124]],\n",
            "\n",
            "        [[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0423,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0091, 0.9909]],\n",
            "\n",
            "        [[0.0099, 0.9901]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0095, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2441, -2.6435]],\n",
            "\n",
            "        [[-1.9660,  2.3659]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]],\n",
            "\n",
            "        [[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0423,  0.9124]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9925, 0.0075]],\n",
            "\n",
            "        [[0.0130, 0.9870]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0103, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.6212,  1.8144]],\n",
            "\n",
            "        [[ 1.9330, -2.6815]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1020, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1490,\n",
            "          -0.9051, -0.0424,  0.9124]],\n",
            "\n",
            "        [[ 0.3151, -1.2576, -1.7395, -0.3163,  0.6259,  0.4211, -0.0456,\n",
            "          -1.0306, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0312, 0.9688]],\n",
            "\n",
            "        [[0.9902, 0.0098]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0208, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4629, -2.4593]],\n",
            "\n",
            "        [[ 2.3434, -2.1680]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3150, -1.2577, -1.7394, -0.3163,  0.6259,  0.4211, -0.0455,\n",
            "          -1.0305, -1.7717, -1.6421]],\n",
            "\n",
            "        [[ 0.3150, -1.2577, -1.7394, -0.3163,  0.6259,  0.4211, -0.0455,\n",
            "          -1.0305, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9928, 0.0072]],\n",
            "\n",
            "        [[0.9891, 0.0109]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0091, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3152,  2.3203]],\n",
            "\n",
            "        [[-2.3125,  2.6031]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1019, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1491,\n",
            "          -0.9048, -0.0425,  0.9123]],\n",
            "\n",
            "        [[-0.1019, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1491,\n",
            "          -0.9048, -0.0425,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0096, 0.9904]],\n",
            "\n",
            "        [[0.0073, 0.9927]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0085, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2445, -2.7432]],\n",
            "\n",
            "        [[ 2.4606, -2.8595]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3150, -1.2577, -1.7394, -0.3163,  0.6259,  0.4211, -0.0455,\n",
            "          -1.0305, -1.7717, -1.6421]],\n",
            "\n",
            "        [[ 0.3150, -1.2577, -1.7394, -0.3163,  0.6259,  0.4211, -0.0455,\n",
            "          -1.0305, -1.7717, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9932, 0.0068]],\n",
            "\n",
            "        [[0.9951, 0.0049]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0058, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2735,  2.3230]],\n",
            "\n",
            "        [[-2.1882,  2.2956]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1019, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1491,\n",
            "          -0.9048, -0.0425,  0.9123]],\n",
            "\n",
            "        [[-0.1019, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1491,\n",
            "          -0.9048, -0.0425,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0100, 0.9900]],\n",
            "\n",
            "        [[0.0112, 0.9888]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0106, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1247, -2.3471]],\n",
            "\n",
            "        [[-1.6539,  2.2391]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3150, -1.2577, -1.7394, -0.3163,  0.6259,  0.4211, -0.0455,\n",
            "          -1.0305, -1.7717, -1.6421]],\n",
            "\n",
            "        [[-0.1019, -0.5995,  0.8418,  2.0118,  0.6632, -1.5419,  0.1491,\n",
            "          -0.9048, -0.0425,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9887, 0.0113]],\n",
            "\n",
            "        [[0.0200, 0.9800]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0158, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.5025,  1.5952]],\n",
            "\n",
            "        [[ 1.8534, -2.5366]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1018, -0.5995,  0.8418,  2.0118,  0.6633, -1.5419,  0.1492,\n",
            "          -0.9046, -0.0427,  0.9123]],\n",
            "\n",
            "        [[ 0.3149, -1.2578, -1.7394, -0.3163,  0.6260,  0.4210, -0.0456,\n",
            "          -1.0305, -1.7716, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0432, 0.9568]],\n",
            "\n",
            "        [[0.9878, 0.0122]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0282, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2248, -2.3245]],\n",
            "\n",
            "        [[ 2.4999, -2.3968]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7394, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0303, -1.7716, -1.6420]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7394, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0303, -1.7716, -1.6420]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9895, 0.0105]],\n",
            "\n",
            "        [[0.9926, 0.0074]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0090, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.9734,  2.3016]],\n",
            "\n",
            "        [[-2.0897,  2.5087]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1017, -0.5994,  0.8418,  2.0117,  0.6634, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[-0.1017, -0.5994,  0.8418,  2.0117,  0.6634, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0137, 0.9863]],\n",
            "\n",
            "        [[0.0100, 0.9900]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0119, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3927, -2.7434]],\n",
            "\n",
            "        [[ 2.4222, -2.7137]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7394, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0303, -1.7716, -1.6420]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7394, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0303, -1.7716, -1.6420]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9942, 0.0058]],\n",
            "\n",
            "        [[0.9942, 0.0058]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0059, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1078,  2.6568]],\n",
            "\n",
            "        [[-2.2835,  2.6331]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1017, -0.5994,  0.8418,  2.0117,  0.6634, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[-0.1017, -0.5994,  0.8418,  2.0117,  0.6634, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0085, 0.9915]],\n",
            "\n",
            "        [[0.0073, 0.9927]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0079, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4326, -2.7348]],\n",
            "\n",
            "        [[-1.7251,  2.3841]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0303, -1.7716, -1.6420]],\n",
            "\n",
            "        [[-0.1017, -0.5994,  0.8418,  2.0117,  0.6634, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9943, 0.0057]],\n",
            "\n",
            "        [[0.0162, 0.9838]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0110, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.8420,  2.3764]],\n",
            "\n",
            "        [[ 2.3760, -2.6056]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1016, -0.5994,  0.8418,  2.0117,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4211, -0.0455,\n",
            "          -1.0304, -1.7716, -1.6420]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0145, 0.9855]],\n",
            "\n",
            "        [[0.9932, 0.0068]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0107, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3041, -2.5094]],\n",
            "\n",
            "        [[ 2.3575, -2.5359]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7716, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7716, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9919, 0.0081]],\n",
            "\n",
            "        [[0.9926, 0.0074]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0078, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.9145,  2.5151]],\n",
            "\n",
            "        [[-2.0137,  2.6484]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1016, -0.5994,  0.8418,  2.0118,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[-0.1016, -0.5994,  0.8418,  2.0118,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0118, 0.9882]],\n",
            "\n",
            "        [[0.0094, 0.9906]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0106, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4032, -2.3238]],\n",
            "\n",
            "        [[ 2.2404, -2.0951]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7716, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7716, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9912, 0.0088]],\n",
            "\n",
            "        [[0.9871, 0.0129]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0109, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3298,  2.5738]],\n",
            "\n",
            "        [[-2.0296,  2.2380]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1016, -0.5994,  0.8418,  2.0118,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[-0.1016, -0.5994,  0.8418,  2.0118,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0074, 0.9926]],\n",
            "\n",
            "        [[0.0138, 0.9862]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0107, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4144, -2.3990]],\n",
            "\n",
            "        [[-1.7438,  2.4256]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]],\n",
            "\n",
            "        [[-0.1016, -0.5994,  0.8418,  2.0118,  0.6635, -1.5419,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9919, 0.0081]],\n",
            "\n",
            "        [[0.0152, 0.9848]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0117, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.9626,  2.6144]],\n",
            "\n",
            "        [[ 2.2395, -2.3893]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0102, 0.9898]],\n",
            "\n",
            "        [[0.9903, 0.0097]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0100, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.2888, -2.3018]],\n",
            "\n",
            "        [[ 2.5425, -2.6132]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3163,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9900, 0.0100]],\n",
            "\n",
            "        [[0.9943, 0.0057]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0079, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3708,  2.5842]],\n",
            "\n",
            "        [[-2.0027,  2.5734]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]],\n",
            "\n",
            "        [[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9123]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0070, 0.9930]],\n",
            "\n",
            "        [[0.0102, 0.9898]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0086, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3488, -2.4395]],\n",
            "\n",
            "        [[ 2.3946, -2.6962]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9917, 0.0083]],\n",
            "\n",
            "        [[0.9939, 0.0061]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0072, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0244,  2.5723]],\n",
            "\n",
            "        [[-2.1809,  2.6441]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9122]],\n",
            "\n",
            "        [[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0100, 0.9900]],\n",
            "\n",
            "        [[0.0080, 0.9920]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0090, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.1937, -2.6837]],\n",
            "\n",
            "        [[-2.2909,  2.2471]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]],\n",
            "\n",
            "        [[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9924, 0.0076]],\n",
            "\n",
            "        [[0.0106, 0.9894]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0091, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-1.6040,  2.2138]],\n",
            "\n",
            "        [[ 2.4008, -2.5438]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5994,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9043, -0.0428,  0.9122]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0304, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0215, 0.9785]],\n",
            "\n",
            "        [[0.9929, 0.0071]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0144, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.4505, -2.8396]],\n",
            "\n",
            "        [[ 2.2822, -2.3067]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9950, 0.0050]],\n",
            "\n",
            "        [[0.9899, 0.0101]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0076, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.0784,  2.4920]],\n",
            "\n",
            "        [[-1.6670,  2.4893]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1493,\n",
            "          -0.9043, -0.0428,  0.9122]],\n",
            "\n",
            "        [[-0.1015, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1493,\n",
            "          -0.9043, -0.0428,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0102, 0.9898]],\n",
            "\n",
            "        [[0.0154, 0.9846]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0129, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 0, 0, 1, 1]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.6367, -2.4618]],\n",
            "\n",
            "        [[ 1.8942, -2.2600]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9939, 0.0061]],\n",
            "\n",
            "        [[0.9845, 0.0155]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0108, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 1, 1, 0, 0]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.3834,  2.5263]],\n",
            "\n",
            "        [[-2.3631,  2.5277]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1015, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1493,\n",
            "          -0.9043, -0.0428,  0.9122]],\n",
            "\n",
            "        [[-0.1015, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1493,\n",
            "          -0.9043, -0.0428,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0073, 0.9927]],\n",
            "\n",
            "        [[0.0075, 0.9925]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0074, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 1, 0, 1, 0, 1]) tensor([[0],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3093, -2.9891]],\n",
            "\n",
            "        [[-2.1973,  2.3542]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7393, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]],\n",
            "\n",
            "        [[-0.1015, -0.5995,  0.8419,  2.0118,  0.6635, -1.5418,  0.1493,\n",
            "          -0.9043, -0.0428,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9950, 0.0050]],\n",
            "\n",
            "        [[0.0104, 0.9896]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0077, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 0, 1, 0, 1, 0]) tensor([[1],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.1315,  2.5903]],\n",
            "\n",
            "        [[ 2.3954, -2.9077]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1014, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9042, -0.0429,  0.9122]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7392, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0088, 0.9912]],\n",
            "\n",
            "        [[0.9950, 0.0050]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0069, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([0, 0, 0, 0, 0, 0]) tensor([[0],\n",
            "        [0]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[ 2.3406, -2.4869]],\n",
            "\n",
            "        [[ 2.5154, -2.8273]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[ 0.3149, -1.2580, -1.7392, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]],\n",
            "\n",
            "        [[ 0.3149, -1.2580, -1.7392, -0.3162,  0.6260,  0.4210, -0.0455,\n",
            "          -1.0305, -1.7715, -1.6421]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.9921, 0.0079]],\n",
            "\n",
            "        [[0.9952, 0.0048]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0064, grad_fn=<NllLoss2DBackward>)\n",
            "tensor([1, 1, 1, 1, 1, 1]) tensor([[1],\n",
            "        [1]])\n",
            "output.shape:  torch.Size([2, 1, 2]) output tensor([[[-2.2267,  2.6927]],\n",
            "\n",
            "        [[-2.3030,  2.6698]]], grad_fn=<AddBackward0>)\n",
            "embed_target:  tensor([[[-0.1014, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9042, -0.0429,  0.9122]],\n",
            "\n",
            "        [[-0.1014, -0.5995,  0.8418,  2.0118,  0.6635, -1.5418,  0.1494,\n",
            "          -0.9042, -0.0429,  0.9122]]], grad_fn=<ViewBackward>)\n",
            "softmax prob:  tensor([[[0.0073, 0.9927]],\n",
            "\n",
            "        [[0.0069, 0.9931]]], grad_fn=<SoftmaxBackward>)\n",
            "loss:  tensor(0.0071, grad_fn=<NllLoss2DBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}